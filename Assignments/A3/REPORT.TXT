CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Xu, Charles

Student Name 2 (last, first): Zhang, Jinming

Student number 1: 1004264271

Student number 2: 1000997503

UTORid 1: xucharle

UTORid 2: zhan2754

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

     Signed: _Charles__  _Jinming Zhang__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
      QLearning: 
             +1 reward for reaching a cheese
             -1 reward for being eaten by cat
      Featured QLearning:  
          features[0]: Path length to closest cheese [0,1)
               it's crucial for the agent to know how far it needs to get to the cheese
          features[1]: Total path length from cheeses to mouse, 
               Knowing shortest path to get all cheeses can also be useful, since we want to get all cheeses as fast as possible
          features[2]: Path length to closest cat (-1,0]
               it's crucial for the agent to know how far it's the closest threat so it can try to avoid it
          features[3]: Total path length from cats to mouse
               the agent also needs to consider all cats and trying to get away from them as much as possible
          features[4]: Path length from closest cheese to it's closest cat
               it's an estimated measurement as how dangerous it is to the target cheese

2 .- These are multiple experiments (once you are sure your 
     QLearning code is working!) 

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 10000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     /////////////////////////////////////////////////////////
     //(...............minimum trials is 100,000...........)//
     //       we used 100,000 trails                       //
     /////////////////////////////////////////////////////////

     Initial mouse winning rate (first rate obtained when training starts):
          success rate=0.063212

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate:
           ~0.73
     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts):
                     success rate=0.066162

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate:
                    ~0.73
     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?     
          The mouse can keep improving, but it will likely to converge to a certain win rate and will not go above that no matter how long it spends on training.
          As the rewards keep accumulating, the optimal policy will be finally stabilized, what is not covered or non-properly rewarded will not be fixed by just increasing training time.

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.
	
     Mouse Winning Rate: ~0.36

     (2 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.
	
     Mouse Winning Rate: ~0.4

     Average rate for Experiement 3 and Experiment 4:
                         ~0.38

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
          The agent learned the optimal policy in for a specific maze environment, it may only works well for that specific maze.
          Once the maze changes, the same policy may not applied to the new environment and thus the agent has a worse performance.


5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts):
               0.04378
     Mouse Winning Rate (from evaluation after training):
               0.044742
     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
          As in a larger maze, the possible states grow exponentially and requires much larger iterations to accumulate rewards and get the optimal policies.
          With the same number of iteration as in the smaller maze, the agent won't be able to gather enough information about all the states and did not perform well.

6 .- (2 marks) Is standard Q-Learning a reasonable strategy for environments
     that change constantly? discuss based on the above
          No, because standard Q-Learning only trained a policy that is good for a specific environment, when environment changes, we have to retrain the agent so 
          it can perform reasonably good in the new environment.
          Also, training the agent takes time, if the environment changes constantly then it's not a good idea to use standard Q-Learning.

7 .- (3 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win
                               Featured QLearning:  
          features[0]: Path length to closest cheese, normalized to [0,1)
               it's crucial for the agent to know how far it needs to get to the cheese
          features[1]: Total path length from cheeses to mouse, normalized to [0,1)
               Knowing shortest path to get all cheeses can also be useful, since we want to get all cheeses as fast as possible
          features[2]: Path length to closest cat, normalized to (-1,0]
               it's crucial for the agent to know how far it's the closest threat so it can try to avoid it
          features[3]: Total path length from cats to mouse, normalized to (-1,0]
               the agent also needs to consider all cats and trying to get away from them as much as possible
          features[4]: Path length from closest cheese to it's closest cat, normalized to [0,1)
               it's an estimated measurement as how dangerous it is to the target cheese


8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):
          0.032230
     Mouse Winning Rate (from evaluation after training):
          0.779460
     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?
           Featured Q-Learning usually has less variables to be trained on (number of features), therefore it takes less iteration to get a reasonable policy than standard Q-Learning.
           However, featured Q-learning captures less information than standard Q-learning on a given environment, so if both are fully trained (result converged), standard Q-learning may 
           perform better.

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):
          ~0.76

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):
          ~0.77

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes 
     to the environment?
          Featured Q-Learning is more suitable for dealing with changing environment, if the features well-captured the properties of the problem, then the weight of these features 
          are not likely have huge difference in different environments for the same problem. So after we trained the weight of these features on one environment, the agent can perform 
          pretty constantly on other environment.

9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):
          0.087741
     Mouse Winning Rate (from evaluation after training):
          ~0.72
     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):
          ~0.71
     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     Mouse Winning Rate (from evaluation after training):
          ~0.76

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs. 
     feature-based Q-learning?
          Based on the result, standard Q-Learning is a better choice when the states of the problem is small and the environment doesn't change often.
          Featured Q-Learning is a better choice when the states of the problem are large or the environment changes constantly.

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?
          Try different set of features that may relevant to the problem on a model and perform reinforcement learning by running simulations.
          Once the result looks good then apply it to real world, we can analyze the real result and enhance our both the simulation module and the reinforcement learning polices.
          Then repeat the simulation -- real world process until the result is satisfiable.
      
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	                                      Partial		Not done

QLearn 
 update               x

Reward              x
 function

Decide
 action          x

featureEval          x
 
evaluateQsa           x

maxQsa_prime        x

Qlearn_features       x

decideAction_features    x

_____________________________________________________

Marking:

(10 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(20 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(25 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 100


